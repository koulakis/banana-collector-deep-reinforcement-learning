{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Navigation\n",
    "Several architectures and hyper-parameter setups were considered for solving the problem. All of them reached the\n",
    "average score of 13+ on 100 consecutive runs, so we will focus here on comparing the most prominent setups based on\n",
    "their top scores and how many episodes they needed to solve the navigation problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network architectures\n",
    "### Comparing the performance of some architectures\n",
    "In the core of each agent was a network which estimated the Q-function. All networks were fully connected networks\n",
    "consisting solely of linear layers and ReLU activations. Batch normalization was briefly tested, but discarded because\n",
    "it didn't offer any improvement. A couple of different setups were considered and tested on 2000 episodes. Their\n",
    "performances were very comparable as one can see in the following figure:\n",
    "![Comparison of network architectures](artifacts/architecture_comparison.png)\n",
    "- The numbers separated by `_` designate the\n",
    "width of each dense layer of the network\n",
    "- The vertical dashed lines mark the first episode where the average score surpassed 13\n",
    "- The reward values are smoothened with a window of legth 100 episodes.\n",
    "\n",
    "Based on the similarity of the results, the network with three hidden layers of widths (64, 64, 64) was picked in order\n",
    "to make sure the performance of setups to be tested is not hindered by a lack of network capacity. This architecture\n",
    "was fix throughout all of the experiments described below.\n",
    "\n",
    "### Dueling DQN\n",
    "A second network is defined which implements the dueling DQN architecture. The output of the last 64 width layer of the\n",
    "(64, 64, 64) network mentioned above is used as input to 2 dense networks which compute respectively the value of the\n",
    "input state and the advantage of the state-action combination. The Q function is then computed as follows:\n",
    "\n",
    "$Q(s, a) = V(s) + (A(s, a) - E_{a'\\in\\mathcal{A}}[A(s, a')])$\n",
    "\n",
    "Both the A and V networks have two layer with sizes (64, 32) and they output respectively $|\\mathcal{A}|$ and 1\n",
    "values.\n",
    "\n",
    "The rest of the training process of the network remains the same and the premise is that the agent will learn both\n",
    "long-term strategies leading to better values (e.g. moving to areas with higher yellow to blue banana ration) and\n",
    "short-term strategies (e.g. collecting yellow bananas and avoiding blue ones), leading to a better performance.\n",
    "\n",
    "For more on dueling DQN, look at the [paper which introduced the method](https://arxiv.org/abs/1511.06581)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training setup\n",
    "### Hyper parameters\n",
    "The hyper parameters directly related to the network training were the following:\n",
    "- learning rate: Check the section below\n",
    "- batch size: This was fixed to 64\n",
    "- hidden layers: This set the number of hidden layers of the backbone fully connected network and their with. After a\n",
    "    short experimentation was fixed to (64, 64, 64)\n",
    "- dueling_dqn: Flag to toggle use of the dueling DQN architecture\n",
    "    \n",
    "### Learning rate\n",
    "It was initially set to 1e-5 but after a couple of experiments, it was evident that the network would benefit from a\n",
    "higher value, at least on the first episodes. This let to using the relatively high rate of 5e-4 in the beginning of\n",
    "training and later on decreasing it on plateaus with a scheduler provided by PyTorch. This step help stabilize a bit\n",
    "the progress of learning on the last steps of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning setup\n",
    "### Hyper parameters\n",
    "The following hyper parameters were used for the Q-learning parts not directly related to the network:\n",
    "- number_of_episodes: This was fixed to 2000 for all experiments and set to 4000 for a final comparion of DQN and its extensions\n",
    "- maximum_timestaps: 1000\n",
    "- initial_epsilon: The exploration rate started from 1.0\n",
    "- final_epsilon: Then decayed to 0.01\n",
    "- epsilon_decay: With a decay rate of 0.995\n",
    "- double_dqn: Flag to toggle use of double DQN\n",
    "- prioritize_replay: Flag to toggle use of priority experience replay\n",
    "- per_alpha: In case prioritized replay is on, set its constant alpha\n",
    "- per_beta_0: In case prioritized replay is on, set the initial value of $\\beta$. $\\beta$ will decay linearly\n",
    "    starting from $\\beta_0$ on the first episode and becoming 1.0 on the last episode.\n",
    "    \n",
    "### Double DQN\n",
    "Double DQN applies a small change to the equation used to estimate the total reward using the Q function. The change\n",
    "is supposed to compensate the overestimation of values given that the max value action is selected for the next step.\n",
    "To compensate for that the index of the max is computed by the local network, while the value by the target network.\n",
    "The equation computing the expected value is:\n",
    "\n",
    "$Y_t = R_t + \\gamma Q(s_{t + 1}, argmax_{a \\in \\mathcal{A}}(Q(s_{t + 1}, a; \\theta)); \\theta^-)$\n",
    "\n",
    "where $\\theta$ are the parameters of the local network, while $\\theta^-$ the parameters of the target network.\n",
    "\n",
    "For more on double DQN, look at the [paper which introduced the method](https://arxiv.org/abs/1509.06461).\n",
    "\n",
    "### Prioritized experience replay\n",
    "Prioritized replay controls how the examples used to train the model are picked. Namely, a probability function defined\n",
    "over all examples gathered in the replay buffer is used and assigns higher probabilities to examples whose Q values\n",
    "estimated using actual rewards deviate more from the Q values computed directly from the Q-network. The implementation\n",
    "involved the introduction of a new buffer which stores priorities along with updates based on the aforementioned errors\n",
    "and also samples based on the distribution defined by them.\n",
    "\n",
    "In order to make sampling efficient, one has to introduce an efficient structure such as a sum tree. This was avoided\n",
    "here in order to save time and was not a real problem given that the number of episodes was fairly low. A couple of\n",
    "cheap caching tricks (mainly on the calculation of the maximum priority) were enough to make the implementation \n",
    "fast enough.\n",
    "\n",
    "For more on prioritized replay, look at the [paper which introduced the method](https://arxiv.org/abs/1511.05952).\n",
    "\n",
    "### CPU vs GPU\n",
    "Given the small size of the networks used and the fact there is a constant conversion from tensors to numpy arrays,\n",
    "it is faster training on cpu, probably because of avoiding intermediate translation steps. If a version of Unity\n",
    "native to PyTorch were available, then probably one could get significant gains using gpus. Of course, in the case\n",
    "where the neural network has a non-trivial architecture e.g. a modern CNN used to learn from pixels, then this\n",
    "overhead would be small compared to the network training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of DQN with extensions\n",
    "Once the backbone network architecture and learning rate were fixed a series of experiments was run on 4000 episode each to evaluate the effect of adding common improvements to the DQN network.\n",
    "The experiment can be thought as a baby version of rainbow, which attempts to analyze the added value when combining several extensions of DQN.\n",
    "\n",
    "### Setups\n",
    "The following experiments were run:\n",
    "- DQN: The initial DQN architecture\n",
    "- Dueling DQN: The dueling DQN architecture\n",
    "- Double, dueling DQN: The dueling DQN with a double expected value function definition\n",
    "- Double, dueling per DQN: All the above using additionally prioritized experience replay\n",
    "\n",
    "### Reward comparison\n",
    "Those are the training reward curves of the above experiments:\n",
    "\n",
    "![Comparison of DQN extensions](artifacts/dqn_improvements_comparison.png)\n",
    "\n",
    "### Conclusions\n",
    "All cominations of improvements exceed the score of 13, but none of them seems to provide a significant advantage in the speed of learning or in the long term performance which is around 17.5. Combined with the fact that the agents behave in a near optimal way when tested live on the environment, this indicates that the problem is too simple for the improvements of DQN to demonstrate an advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example test run of a trained DQN agent\n",
    "Below we can see a giff with the behavior of a train agent using a dqn network. This is a randomly selected environment which is not cherry-picked. To reproduce, one can run the following script:\n",
    "\n",
    "`python scripts/test_agent_in_environment.py --agent-parameters-path experiments/final_comparison_dqn/checkpoint.pth --no-dueling-dqn`\n",
    "\n",
    "![Agent test run](artifacts/screencast_unity_edited.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future ideas to improve performance\n",
    "Here are some ideas on how to improve performance.\n",
    "\n",
    "### Error analysis\n",
    "This would be the first step in order to figure out what is wrong in the agent's behaviour. One could e.g. test the agent on several episodes and watch the\n",
    "worst ones to figure out what went wrong. If it is a lack of planning, one could look of improvements which help the agent plan better or trying out a deeper\n",
    "network which has more capacity or even adding some memory to the agent (e.g. use an RNN). If on the other hand the agent has problems on short-term actions,\n",
    "one could look into methods similar to dueling DQN which add extra focus on that. Finally, it could be the case (which actuall seems like that) that the agent\n",
    "has a near optimal behavior and it is not worth it trying to further optimize its training.\n",
    "\n",
    "In any case, the error analysis would give a direction to the next investigations and also a feeling of how hard/easy each direction could be.\n",
    "\n",
    "### Use a higher level framework\n",
    "It is a nice exercise writing all those algorithms from scratch, but if one where to focus on optimizing such systems, one should turn to some popular framework\n",
    "in the field of reinforcement learning which has most SOTA algorithms implemented. \n",
    "\n",
    "Some popular frameworks seem to be:\n",
    "- [ReAgent](https://reagent.ai/)\n",
    "- [Dopamine](https://github.com/google/dopamine/tree/master/docs)\n",
    "\n",
    "### Rainbow\n",
    "Rainbow combines most of the recent proposed reinforcement learning methods in one algorithm and seems to be a quite sucessful and established architecture.\n",
    "It could be a good candidate to apply to this problem.\n",
    "\n",
    "### Learn from pixels\n",
    "If the error analysis shows that there are errors due to lack of information in the ray based perception, one could try getting the image input and train a model\n",
    "from pixels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
